{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1835750,"sourceType":"datasetVersion","datasetId":943374},{"sourceId":13534960,"sourceType":"datasetVersion","datasetId":8594689}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport random\n\nnigeria_images_path = \"/kaggle/input/satellite-images-to-predict-povertyafrica/nigeria_archive/images\"\n\nif os.path.exists(nigeria_images_path):\n    image_files = [f for f in os.listdir(nigeria_images_path) if f.endswith('.png')]\n    num_images = len(image_files)\n    print(f\"Total number of images: {num_images}\")\n    \n    if num_images >= 3:\n        sample_images = random.sample(image_files, 3)\n        plt.figure(figsize=(12, 4))\n        for i, img_name in enumerate(sample_images):\n            img_path = os.path.join(nigeria_images_path, img_name)\n            img = Image.open(img_path)\n            plt.subplot(1, 3, i + 1)\n            plt.imshow(img)\n            plt.axis('off')\n            plt.title(f\"Sample {i+1}\")\n        plt.show()\n    else:\n        print(\"There are less than 3 images in the directory.\")\nelse:\n    print(f\"Directory {nigeria_images_path} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:32.869556Z","iopub.execute_input":"2025-10-28T18:03:32.869963Z","iopub.status.idle":"2025-10-28T18:03:32.879773Z","shell.execute_reply.started":"2025-10-28T18:03:32.869935Z","shell.execute_reply":"2025-10-28T18:03:32.878638Z"}},"outputs":[{"name":"stdout","text":"Directory /kaggle/input/satellite-images-to-predict-povertyafrica/nigeria_archive/images does not exist.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nimage_dir = \"/kaggle/input/satellite-images-to-predict-povertyafrica/nigeria_archive/images\"\nnormalized_images = []\nresize_dim = (64, 64)\n\nfile_names = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n\nfor file_name in tqdm(file_names, desc=\"Processing images\", ncols=100):\n    file_path = os.path.join(image_dir, file_name)\n    img = cv2.imread(file_path)\n    if img is not None:\n        img_resized = cv2.resize(img, resize_dim)\n        img_normalized = img_resized / 255.0\n        normalized_images.append(img_normalized)\n\nnormalized_images = np.array(normalized_images)\n\nprint(f\"Number of normalized images: {len(normalized_images)}\")\nprint(f\"Dimensions of normalized images: {normalized_images.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:32.881235Z","iopub.execute_input":"2025-10-28T18:03:32.881487Z","iopub.status.idle":"2025-10-28T18:03:32.960982Z","shell.execute_reply.started":"2025-10-28T18:03:32.881467Z","shell.execute_reply":"2025-10-28T18:03:32.959575Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3233664467.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresize_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/satellite-images-to-predict-povertyafrica/nigeria_archive/images'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/satellite-images-to-predict-povertyafrica/nigeria_archive/images'","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchvision import models, transforms\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nimage_dir = \"/kaggle/input/satellite-images-to-predict-povertyafrica/nigeria_archive/images\"\nfile_names = [f for f in os.listdir(image_dir) if f.endswith('.png')]\n\n# Load pre-trained ResNet50\nmodel = models.resnet50(pretrained=True)\nmodel = model.to(device)\nmodel.eval()\n\nfeatures = []\nresize_dim = (224, 224)\n\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(resize_dim),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nfor file_name in tqdm(file_names, desc=\"Extracting features\", ncols=100):\n    file_path = os.path.join(image_dir, file_name)\n    img = cv2.imread(file_path)\n    if img is not None:\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_transformed = transform(img_rgb)\n        img_transformed = img_transformed.unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            feature = model(img_transformed)\n        features.append(feature.cpu().numpy().flatten())\n\nfeatures = np.array(features)\nprint(f\"Number of feature vectors extracted: {len(features)}\")\nprint(f\"Feature dimensions: {features.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:32.961424Z","iopub.status.idle":"2025-10-28T18:03:32.961719Z","shell.execute_reply.started":"2025-10-28T18:03:32.961585Z","shell.execute_reply":"2025-10-28T18:03:32.961598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ninertias = []\nk_range = range(1, 11)\n\nfor k in tqdm(k_range, desc=\"Finding optimal clusters\"):\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(features)\n    inertias.append(kmeans.inertia_)\n\n# Plot elbow curve\nplt.figure(figsize=(10, 6))\nplt.plot(k_range, inertias, marker='o', linestyle='-', color='b')\nplt.title(\"Elbow Method for Optimal Number of Clusters\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia\")\nplt.xticks(k_range)\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:32.962898Z","iopub.status.idle":"2025-10-28T18:03:32.963163Z","shell.execute_reply.started":"2025-10-28T18:03:32.963033Z","shell.execute_reply":"2025-10-28T18:03:32.963044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Based on elbow method, choose k=3\nk = 3\nkmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(features)\n\ncluster_counts = np.bincount(cluster_labels)\nfor i, count in enumerate(cluster_counts):\n    print(f\"Cluster {i}: {count} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:32.964572Z","iopub.status.idle":"2025-10-28T18:03:32.964956Z","shell.execute_reply.started":"2025-10-28T18:03:32.964759Z","shell.execute_reply":"2025-10-28T18:03:32.964777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nclasses = {0: \"Poor\", 1: \"Rich\", 2: \"Middle Class\"}\ncluster_counts = np.bincount(cluster_labels)\n\nclass_names = [classes.get(i, f\"Class {i}\") for i in range(len(cluster_counts))]\ncounts = cluster_counts\n\nplt.figure(figsize=(10, 6))\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\nplt.bar(class_names, counts, color=colors, alpha=0.8)\nplt.title(\"Distribution of Images by Poverty Class\", fontsize=14, fontweight='bold')\nplt.xlabel(\"Poverty Class\", fontsize=12)\nplt.ylabel(\"Number of Images\", fontsize=12)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add value labels on bars\nfor i, v in enumerate(counts):\n    plt.text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:32.965868Z","iopub.status.idle":"2025-10-28T18:03:32.966175Z","shell.execute_reply.started":"2025-10-28T18:03:32.966043Z","shell.execute_reply":"2025-10-28T18:03:32.966056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\n\nfig, axes = plt.subplots(k, 3, figsize=(15, 12))\n\nfor cluster_num in range(k):\n    cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_num]\n    \n    # Take up to 3 samples from each cluster\n    sample_indices = cluster_indices[:3] if len(cluster_indices) >= 3 else cluster_indices\n    \n    for i, idx in enumerate(sample_indices):\n        if i < 3:  # Ensure we don't exceed 3 columns\n            file_path = os.path.join(image_dir, file_names[idx])\n            img = cv2.imread(file_path)\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            axes[cluster_num, i].imshow(img_rgb)\n            axes[cluster_num, i].axis('off')\n            if i == 1:  # Title only on middle image\n                axes[cluster_num, i].set_title(f\"{classes[cluster_num]} (Cluster {cluster_num})\", \n                                             fontweight='bold', fontsize=12)\n\n# Remove empty subplots\nfor i in range(len(sample_indices), 3):\n    for cluster_num in range(k):\n        axes[cluster_num, i].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:33.135411Z","iopub.execute_input":"2025-10-28T18:03:33.136491Z","iopub.status.idle":"2025-10-28T18:03:33.151481Z","shell.execute_reply.started":"2025-10-28T18:03:33.136460Z","shell.execute_reply":"2025-10-28T18:03:33.149950Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/993166030.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"],"ename":"NameError","evalue":"name 'k' is not defined","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Prepare data for CNN\nlabels = cluster_labels\n\n# Convert to PyTorch tensors\nnormalized_images_tensor = torch.tensor(normalized_images, dtype=torch.float32)\nlabels_tensor = torch.tensor(labels, dtype=torch.long)\n\n# Permute to (batch_size, channels, height, width)\nnormalized_images_tensor = normalized_images_tensor.permute(0, 3, 1, 2)\n\n# Split data\nX_train, X_temp, y_train, y_temp = train_test_split(\n    normalized_images_tensor, labels_tensor, test_size=0.3, random_state=42\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=0.5, random_state=42\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\n\n# Create data loaders\nbatch_size = 32\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset = TensorDataset(X_val, y_val)\ntest_dataset = TensorDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define CNN model\nclass PovertyCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(PovertyCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(128 * 8 * 8, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = self.pool(torch.relu(self.conv3(x)))\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Initialize model\nnum_classes = len(np.unique(labels))\nmodel = PovertyCNN(num_classes).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 50\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    running_loss = 0.0\n    for images, targets in train_loader:\n        images, targets = images.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item()\n            \n            _, predicted = torch.max(outputs, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n    \n    train_loss = running_loss / len(train_loader)\n    val_loss_avg = val_loss / len(val_loader)\n    val_accuracy = 100 * correct / total\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss_avg)\n    val_accuracies.append(val_accuracy)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, \"\n          f\"Val Loss: {val_loss_avg:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\nprint(\"Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:33.152083Z","iopub.status.idle":"2025-10-28T18:03:33.152434Z","shell.execute_reply.started":"2025-10-28T18:03:33.152266Z","shell.execute_reply":"2025-10-28T18:03:33.152281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Evaluate on test set\nmodel.eval()\nall_preds = []\nall_labels = []\nall_probabilities = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        probabilities = torch.softmax(outputs, dim=1)\n        _, predicted = torch.max(outputs, 1)\n        \n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        all_probabilities.extend(probabilities.cpu().numpy())\n\n# Convert to numpy arrays\nall_preds = np.array(all_preds)\nall_labels = np.array(all_labels)\nall_probabilities = np.array(all_probabilities)\n\n# Calculate metrics\naccuracy = accuracy_score(all_labels, all_preds)\nconf_matrix = confusion_matrix(all_labels, all_preds)\nclass_report = classification_report(all_labels, all_preds, \n                                   target_names=[classes[i] for i in range(num_classes)])\n\nprint(\"=\" * 50)\nprint(\"MODEL EVALUATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Overall Accuracy: {accuracy:.4f}\")\nprint(f\"Overall Accuracy (%): {accuracy * 100:.2f}%\")\nprint(\"\\\\nClassification Report:\")\nprint(class_report)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n            xticklabels=[classes[i] for i in range(num_classes)],\n            yticklabels=[classes[i] for i in range(num_classes)])\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.tight_layout()\nplt.show()\n\n# Plot training history\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss', color='blue')\nplt.plot(val_losses, label='Validation Loss', color='red')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracies, label='Validation Accuracy', color='green')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy (%)')\nplt.title('Validation Accuracy')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:33.154036Z","iopub.status.idle":"2025-10-28T18:03:33.154541Z","shell.execute_reply.started":"2025-10-28T18:03:33.154274Z","shell.execute_reply":"2025-10-28T18:03:33.154295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'classes': classes,\n    'num_classes': num_classes,\n    'model_architecture': model.__class__.__name__\n}, 'poverty_cnn_model.pth')\n\nprint(\"Model saved successfully!\")\nprint(\"Model file: 'poverty_cnn_model.pth'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:33.155793Z","iopub.status.idle":"2025-10-28T18:03:33.156232Z","shell.execute_reply.started":"2025-10-28T18:03:33.155986Z","shell.execute_reply":"2025-10-28T18:03:33.156006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ipywidgets as widgets\nfrom IPython.display import display, clear_output\nimport torch\nfrom PIL import Image as PILImage\nimport io\nimport numpy as np\n\n# Load the saved model for inference\ndef load_model_for_inference():\n    checkpoint = torch.load('poverty_cnn_model.pth', map_location=device)\n    model = PovertyCNN(checkpoint['num_classes'])\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n    return model, checkpoint['classes']\n\n# Load model\ninference_model, class_names = load_model_for_inference()\n\n# Preprocessing function for new images\ndef preprocess_image(uploaded_image):\n    # Convert to PIL Image\n    image = PILImage.open(io.BytesIO(uploaded_image))\n    \n    # Convert to numpy array\n    img_array = np.array(image)\n    \n    # If image has 4 channels (RGBA), convert to 3 channels (RGB)\n    if img_array.shape[-1] == 4:\n        img_array = img_array[:, :, :3]\n    \n    # Resize and normalize\n    img_resized = cv2.resize(img_array, (64, 64))\n    img_normalized = img_resized / 255.0\n    \n    # Convert to tensor\n    img_tensor = torch.tensor(img_normalized, dtype=torch.float32)\n    img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0)  # (1, 3, 64, 64)\n    \n    return img_tensor, img_array\n\n# Prediction function\ndef predict_poverty(uploaded_image):\n    # Preprocess image\n    img_tensor, original_img = preprocess_image(uploaded_image)\n    img_tensor = img_tensor.to(device)\n    \n    # Make prediction\n    with torch.no_grad():\n        output = inference_model(img_tensor)\n        probabilities = torch.softmax(output, dim=1)\n        predicted_class = torch.argmax(output, dim=1).item()\n        confidence = torch.max(probabilities).item()\n    \n    return predicted_class, confidence, probabilities.cpu().numpy()[0], original_img\n\n# Create widgets\nupload_button = widgets.FileUpload(\n    description='Upload Image',\n    accept='.png,.jpg,.jpeg',\n    multiple=False\n)\n\npredict_button = widgets.Button(\n    description='Predict Poverty Level',\n    button_style='success',\n    tooltip='Click to make prediction'\n)\n\noutput_area = widgets.Output()\n\n# Prediction handler - FIXED VERSION\ndef on_predict_button_clicked(b):\n    with output_area:\n        clear_output()\n        if upload_button.value:\n            try:\n                # Get uploaded image - FIXED: Handle different return formats\n                uploaded_data = upload_button.value\n                \n                # Handle different versions of ipywidgets\n                if isinstance(uploaded_data, dict):\n                    # Older version - dict format\n                    uploaded_image = list(uploaded_data.values())[0]['content']\n                elif isinstance(uploaded_data, tuple) and len(uploaded_data) > 0:\n                    # Newer version - tuple format\n                    if hasattr(uploaded_data[0], 'content'):\n                        uploaded_image = uploaded_data[0].content\n                    else:\n                        uploaded_image = uploaded_data[0]['content']\n                else:\n                    # Try to handle other formats\n                    uploaded_image = uploaded_data[0].content if hasattr(uploaded_data[0], 'content') else uploaded_data[0]['content']\n                \n                # Make prediction\n                predicted_class, confidence, probabilities, original_img = predict_poverty(uploaded_image)\n                \n                # Display results\n                plt.figure(figsize=(12, 5))\n                \n                # Display original image\n                plt.subplot(1, 2, 1)\n                plt.imshow(original_img)\n                plt.axis('off')\n                plt.title('Uploaded Image', fontweight='bold', fontsize=12)\n                \n                # Display prediction results\n                plt.subplot(1, 2, 2)\n                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n                classes_list = [class_names[i] for i in range(len(class_names))]\n                \n                bars = plt.bar(classes_list, probabilities, color=colors, alpha=0.8)\n                plt.ylim(0, 1)\n                plt.ylabel('Probability', fontsize=12)\n                plt.title('Poverty Level Prediction Probabilities', fontweight='bold', fontsize=12)\n                \n                # Add value labels on bars\n                for bar, prob in zip(bars, probabilities):\n                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n                            f'{prob:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n                \n                plt.grid(axis='y', alpha=0.3)\n                plt.xticks(rotation=45)\n                plt.tight_layout()\n                plt.show()\n                \n                # Print prediction summary\n                print(\"=\" * 60)\n                print(\"üéØ POVERTY PREDICTION RESULTS\")\n                print(\"=\" * 60)\n                print(f\"üè† Predicted Poverty Level: {class_names[predicted_class]}\")\n                print(f\"üìä Confidence: {confidence:.3f} ({confidence*100:.1f}%)\")\n                print(f\"üî¢ Class ID: {predicted_class}\")\n                print(\"\\nProbability Distribution:\")\n                for i, prob in enumerate(probabilities):\n                    print(f\"  üìà {class_names[i]}: {prob:.3f} ({prob*100:.1f}%)\")\n                print(\"=\" * 60)\n                \n                # Add interpretation\n                if confidence > 0.7:\n                    print(\"‚úÖ High confidence prediction\")\n                elif confidence > 0.5:\n                    print(\"‚ö†Ô∏è  Moderate confidence prediction\")\n                else:\n                    print(\"‚ùì Low confidence prediction - consider manual review\")\n                    \n            except Exception as e:\n                print(f\"‚ùå Error processing image: {str(e)}\")\n                print(\"Please try uploading a different image.\")\n        else:\n            print(\"‚ùå Please upload an image first!\")\n\n# Connect button to handler\npredict_button.on_click(on_predict_button_clicked)\n\n# Display the interface\nprint(\"üéØ SATELLITE IMAGE POVERTY PREDICTION INTERFACE\")\nprint(\"Upload a satellite image to predict poverty level\")\nprint(\"=\" * 60)\nprint(\"Supported formats: PNG, JPG, JPEG\")\nprint(\"Classes: Poor, Middle Class, Rich\")\nprint(\"=\" * 60)\n\n# Layout\nvbox = widgets.VBox([\n    widgets.HBox([upload_button, predict_button]),\n    output_area\n])\n\ndisplay(vbox)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:33.157607Z","iopub.status.idle":"2025-10-28T18:03:33.157999Z","shell.execute_reply.started":"2025-10-28T18:03:33.157804Z","shell.execute_reply":"2025-10-28T18:03:33.157821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the prediction with some sample images from the dataset\nprint(\"Testing prediction with sample images...\")\nprint(\"=\" * 50)\n\n# Get one sample from each class\nsample_predictions = []\nfor cluster_num in range(k):\n    cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_num]\n    if cluster_indices:\n        sample_idx = cluster_indices[0]\n        sample_path = os.path.join(image_dir, file_names[sample_idx])\n        \n        # Read and process the image\n        with open(sample_path, 'rb') as f:\n            image_data = f.read()\n        \n        predicted_class, confidence, probabilities, original_img = predict_poverty(image_data)\n        \n        sample_predictions.append({\n            'true_class': cluster_num,\n            'predicted_class': predicted_class,\n            'confidence': confidence,\n            'image': original_img\n        })\n\n# Display sample predictions\nfig, axes = plt.subplots(1, k, figsize=(15, 5))\nif k == 1:\n    axes = [axes]\n\nfor i, prediction in enumerate(sample_predictions):\n    axes[i].imshow(prediction['image'])\n    axes[i].axis('off')\n    true_class_name = class_names[prediction['true_class']]\n    pred_class_name = class_names[prediction['predicted_class']]\n    status = \"‚úì\" if prediction['true_class'] == prediction['predicted_class'] else \"‚úó\"\n    axes[i].set_title(f\"True: {true_class_name}\\\\nPred: {pred_class_name}\\\\nConf: {prediction['confidence']:.3f} {status}\", \n                     fontweight='bold')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T18:03:33.161787Z","iopub.execute_input":"2025-10-28T18:03:33.162114Z","iopub.status.idle":"2025-10-28T18:03:33.179439Z","shell.execute_reply.started":"2025-10-28T18:03:33.162071Z","shell.execute_reply":"2025-10-28T18:03:33.178009Z"}},"outputs":[{"name":"stdout","text":"Testing prediction with sample images...\n==================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1169707147.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get one sample from each class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msample_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcluster_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcluster_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcluster_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcluster_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'k' is not defined"],"ename":"NameError","evalue":"name 'k' is not defined","output_type":"error"}],"execution_count":21},{"cell_type":"markdown","source":"# Attempt 1 - Streamlit app with joblib file in ngrok server","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\n\nimport streamlit as st\nimport os\nimport cv2\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport joblib\n\n# --- 1. Load Pre-computed Model and Set Up ---\nst.title(\"Poverty Prediction from Satellite Images\")\n\n@st.cache_resource\ndef load_prediction_pipeline():\n    # Load the trained KMeans model\n    model_path = \"/kaggle/input/poverty-prediction/kmeans_model (1).joblib\"\n    kmeans_model = joblib.load(model_path)\n    \n    # --- DEFINITIVE FIX for DTYPE MISMATCH ---\n    # The error is inside the model file itself. We force its internal\n    # cluster centers to be the correct data type (float64/double).\n    kmeans_model.cluster_centers_ = kmeans_model.cluster_centers_.astype(np.float64)\n    # --- END OF FIX ---\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load the ResNet50 model for feature extraction\n    feature_extractor = models.resnet50(weights='IMAGENET1K_V1')\n    feature_extractor = feature_extractor.to(device)\n    feature_extractor.eval()\n    \n    transform_pipeline = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    return kmeans_model, feature_extractor, transform_pipeline, device\n\nwith st.spinner('Loading prediction model...'):\n    kmeans, model, transform, device = load_prediction_pipeline()\nst.success(\"Model loaded successfully!\")\n\nclasses = {0: \"Poor\", 1: \"Rich\", 2: \"Middle Class\"} \n\n# --- 2. Main Streamlit Application Logic ---\nst.header(\"Upload an Image for Prediction\")\nuploaded_file = st.file_uploader(\"Choose a satellite image...\", type=\"png\")\n\nif uploaded_file is not None:\n    image = Image.open(uploaded_file)\n    \n    # FIX: Update for Streamlit deprecation warning\n    st.image(image, caption='Uploaded Image.', width=None) # Using default width handling\n\n    with st.spinner('Analyzing the image...'):\n        img_array = np.array(image)\n\n        # Handle different image formats (Grayscale, RGBA, etc.)\n        if len(img_array.shape) < 3 or img_array.shape[2] == 1:\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n        elif img_array.shape[2] == 4:\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)\n        \n        img_transformed = transform(img_array).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            feature = model(img_transformed)\n            \n        # Ensure input feature is also float64 for consistency\n        feature_final = feature.cpu().numpy().flatten().astype(np.float64)\n        \n        prediction = kmeans.predict([feature_final])[0]\n\n    st.write(f\"### Predicted Poverty Level: **{classes[prediction]}**\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install required libraries, forcing the correct scikit-learn version\n!pip install streamlit -q\n!pip install pyngrok -q\n!pip install scikit-learn==1.2.2\n\n# Get the ngrok authtoken from secrets\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nngrok_token = user_secrets.get_secret(\"NGROK_AUTHTOKEN\")\n\n# Launch ngrok\nfrom pyngrok import ngrok\nngrok.set_auth_token(ngrok_token)\npublic_url = ngrok.connect(8501)\nprint(f\"‚úÖ Your app is live! Click here: {public_url}\")\n\n# Run the streamlit app\n!streamlit run app.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:45:07.581491Z","iopub.execute_input":"2025-11-06T15:45:07.581840Z","iopub.status.idle":"2025-11-06T16:02:57.466120Z","shell.execute_reply.started":"2025-11-06T15:45:07.581820Z","shell.execute_reply":"2025-11-06T16:02:57.464694Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\n‚úÖ Your app is live! Click here: NgrokTunnel: \"https://nonhistrionically-loudish-latosha.ngrok-free.dev\" -> \"http://localhost:8501\"\n\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.90.49.39:8501\u001b[0m\n\u001b[0m\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 164MB/s]\n\u001b[31m‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ\u001b[0m\n\u001b[31m \u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m 129 in exec_func_with_error_handling                                                 \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1mscript_runner\u001b[0m \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[1m.py\u001b[0m:669 in code_to_exec                                                              \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2m/kaggle/working/\u001b[0m\u001b[1mapp.py\u001b[0m:56 in <module>                                                \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m53 \u001b[0m\u001b[2m‚îÇ   \u001b[0mimage = Image.open(uploaded_file)                                           \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m54 \u001b[0m\u001b[2m‚îÇ   \u001b[0m                                                                            \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[2m# FIX: Update for Streamlit deprecation warning\u001b[0m                             \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[31m‚ù± \u001b[0m56 \u001b[2m‚îÇ   \u001b[0m\u001b[1;4mst.image(image, caption=\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mUploaded Image.\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4m, width=\u001b[0m\u001b[1;4;94mNone\u001b[0m\u001b[1;4m)\u001b[0m \u001b[2m# Using default widt\u001b[0m \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m57 \u001b[0m\u001b[2m‚îÇ   \u001b[0m                                                                            \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m‚îÇ   \u001b[0m\u001b[94mwith\u001b[0m st.spinner(\u001b[33m'\u001b[0m\u001b[33mAnalyzing the image...\u001b[0m\u001b[33m'\u001b[0m):                                  \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m   \u001b[2m59 \u001b[0m\u001b[2m‚îÇ   ‚îÇ   \u001b[0mimg_array = np.array(image)                                             \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/\u001b[0m\u001b[1mmetrics_util.py\u001b[0m:447 in     \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m wrapped_func                                                                         \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/streamlit/elements/\u001b[0m\u001b[1mimage.py\u001b[0m:202 in image     \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m \u001b[2m/usr/local/lib/python3.11/dist-packages/streamlit/elements/lib/\u001b[0m\u001b[1mlayout_utils.py\u001b[0m:73 in \u001b[31m \u001b[0m\n\u001b[31m \u001b[0m validate_width                                                                       \u001b[31m \u001b[0m\n\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n\u001b[1;91mStreamlitInvalidWidthError: \u001b[0mInvalid width value: \u001b[3;35mNone\u001b[0m. Width must be either an integer \n\u001b[1m(\u001b[0mpixels\u001b[1m)\u001b[0m, \u001b[32m'stretch'\u001b[0m, or \u001b[32m'content'\u001b[0m.\n^C\n\u001b[34m  Stopping...\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Attempt 2 - Direct run in notebook","metadata":{}},{"cell_type":"code","source":"!pip install ipywidgets -q\n!pip install scikit-learn==1.2.2\n!pip install torchvision torch -q\n\nimport os\nimport cv2\nimport torch\nimport numpy as np\nimport joblib\nimport ipywidgets as widgets\nfrom PIL import Image\nfrom torchvision import models, transforms\nfrom IPython.display import display, clear_output\nimport io\nimport matplotlib.pyplot as plt  # <-- ADD THIS LINE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:31:14.536387Z","iopub.execute_input":"2025-11-06T16:31:14.537907Z","iopub.status.idle":"2025-11-06T16:31:26.957943Z","shell.execute_reply.started":"2025-11-06T16:31:14.537859Z","shell.execute_reply":"2025-11-06T16:31:26.956296Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Create an output widget to show loading status\nload_output = widgets.Output()\n\n@load_output.capture(wait=True)\ndef load_model_pipeline():\n    \"\"\"Loads the ML pipeline and fixes data types.\"\"\"\n    print(\"Loading prediction pipeline...\")\n    \n    # Load the trained KMeans model\n    model_path = \"/kaggle/input/poverty-prediction/kmeans_model (1).joblib\"\n    kmeans_model = joblib.load(model_path)\n    \n    # --- DEFINITIVE FIX 1: Fix the model's internal data type ---\n    # This forces the model's cluster centers to be float64 ('double')\n    kmeans_model.cluster_centers_ = kmeans_model.cluster_centers_.astype(np.float64)\n    \n    # Load the ResNet50 feature extractor\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    feature_extractor = models.resnet50(weights='IMAGENET1K_V1').to(device).eval()\n    \n    # Define the image transformation\n    transform_pipeline = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    print(\"Pipeline loaded successfully!\")\n    return kmeans_model, feature_extractor, transform_pipeline, device\n\n# Display the loading output\ndisplay(load_output)\n\n# Run the loading function\nkmeans, model, transform, device = load_model_pipeline()\n\n# Define class labels\nclasses = {0: \"Poor\", 1: \"Rich\", 2: \"Middle Class\"}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:20:16.656885Z","iopub.execute_input":"2025-11-06T16:20:16.657787Z","iopub.status.idle":"2025-11-06T16:20:18.842768Z","shell.execute_reply.started":"2025-11-06T16:20:16.657750Z","shell.execute_reply":"2025-11-06T16:20:18.841874Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b535aa3ef1427d93fae3aa5b53d50e"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def predict_image(image_bytes):\n    \"\"\"Runs a single image through the prediction pipeline.\"\"\"\n    try:\n        # Open image from bytes\n        image = Image.open(io.BytesIO(image_bytes))\n        \n        # Convert to numpy array and handle channels\n        img_array = np.array(image)\n        if len(img_array.shape) < 3 or img_array.shape[2] == 1:\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n        elif img_array.shape[2] == 4:\n            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)\n            \n        # Apply transforms\n        img_transformed = transform(img_array).unsqueeze(0).to(device)\n        \n        # Extract features\n        with torch.no_grad():\n            feature = model(img_transformed)\n            \n        # --- Fix the input data type ---\n        feature_final = feature.cpu().numpy().flatten().astype(np.float64)\n        \n        # --- Get distances to ALL clusters ---\n        distances = kmeans.transform([feature_final])[0]\n        \n        # Make prediction\n        prediction_index = np.argmin(distances)\n        prediction_class = classes[prediction_index]\n        \n        return prediction_class, distances, img_array\n        \n    except Exception as e:\n        return f\"Error: {e}\", None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:31:39.769961Z","iopub.execute_input":"2025-11-06T16:31:39.770377Z","iopub.status.idle":"2025-11-06T16:31:39.780604Z","shell.execute_reply.started":"2025-11-06T16:31:39.770352Z","shell.execute_reply":"2025-11-06T16:31:39.779348Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Create the file uploader\nuploader = widgets.FileUpload(\n    accept='image/*',  # Only accept image files\n    description='Upload Image'\n)\n\n# Create an output area\noutput_area = widgets.Output()\n\ndef on_upload(change):\n    \"\"\"This function runs when a new file is uploaded.\"\"\"\n    with output_area:\n        # Clear previous output\n        clear_output(wait=True) \n        \n        # Get the uploaded file info\n        uploaded_file = uploader.value\n        if not uploaded_file:\n            return\n            \n        # Get the first file from the upload\n        file_info = uploaded_file[-1]\n        file_name = file_info['name']\n        image_bytes = file_info['content']\n        \n        # Display the uploaded image\n        img_pil = Image.open(io.BytesIO(image_bytes))\n        display(img_pil)\n        \n        # Run prediction\n        print(\"Analyzing...\")\n        predicted_class, distances, img_array = predict_image(image_bytes)\n        \n        if predicted_class == \"Error:\":\n            print(distances) # Print the error message\n            return\n            \n        # --- 1. Image Analytics ---\n        print(\"\\n--- Image Analytics ---\")\n        print(f\"File Name: {file_name}\")\n        print(f\"Dimensions: {img_pil.width} x {img_pil.height} pixels\")\n        print(f\"File Size: {len(image_bytes) / 1024:.1f} KB\")\n\n        # Create a color histogram\n        plt.figure(figsize=(6, 2))\n        colors = ('r', 'g', 'b')\n        for i, col in enumerate(colors):\n            hist = cv2.calcHist([img_array], [i], None, [256], [0, 256])\n            plt.plot(hist, color=col)\n        plt.title(\"Image Color Histogram\")\n        plt.xlim([0, 256])\n        plt.yticks([])\n        plt.show()\n\n        # --- 2. Prediction Analytics ---\n        print(\"\\n--- Prediction Analytics ---\")\n        \n        # Calculate a simple \"confidence\" score.\n        # Lower distance = higher confidence.\n        # We invert and normalize to get percentages.\n        confidence_scores = 1.0 / (distances + 1e-6) # Add epsilon to avoid division by zero\n        confidence_percent = (confidence_scores / np.sum(confidence_scores)) * 100\n        \n        print(\"Model Confidence (based on distance to cluster centers):\")\n        for i, label in classes.items():\n            is_prediction = \" (Prediction)\" if label == predicted_class else \"\"\n            print(f\"  - Cluster {label}: {confidence_percent[i]:.1f}% (Distance: {distances[i]:.2f}){is_prediction}\")\n\n        print(f\"\\n--- Final Prediction: {predicted_class} ---\")\n\n# Link the uploader to the function\nuploader.observe(on_upload, names='value')\n\n# Display the UI\nprint(\"Poverty Prediction Interface:\")\ndisplay(uploader, output_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:31:54.834743Z","iopub.execute_input":"2025-11-06T16:31:54.835103Z","iopub.status.idle":"2025-11-06T16:31:54.856627Z","shell.execute_reply.started":"2025-11-06T16:31:54.835078Z","shell.execute_reply":"2025-11-06T16:31:54.855513Z"}},"outputs":[{"name":"stdout","text":"Poverty Prediction Interface:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FileUpload(value=(), accept='image/*', description='Upload Image')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012d85c6fa1944e3893aea83880c8759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"628a66ca9d18422a84180bac6558f755"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}